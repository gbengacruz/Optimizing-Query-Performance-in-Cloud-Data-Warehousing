**A Comparative Analysis of Microsoft Azure Synapse, Amazon Redshift, and Oracle Autonomous Data Warehouse**

---

## ğŸ“Œ Project Overview

Cloud data warehouses are now central to modern analytics, yet organisationsâ€”particularly SMEsâ€”often struggle to choose the most suitable platform due to conflicting benchmark results and unclear performance guidance.

This research provides a **neutral, controlled, and empirical comparison** of three leading cloud data warehousing platforms:

* **Microsoft Azure Synapse Analytics**
* **Amazon Redshift**
* **Oracle Autonomous Data Warehouse (ADW)**

Using industry-standard **TPC-DS and TPC-H benchmarks (10GB scale)**, the study evaluates baseline performance, optimization effectiveness, and concurrency behaviour under SME-representative constraints.

---

## ğŸ¯ Research Objectives

* Benchmark Azure Synapse, Amazon Redshift, and Oracle ADW under identical conditions
* Evaluate platform-specific optimization strategies (compression, distribution keys, materialized views)
* Analyse scalability and concurrency behaviour
* Provide an **evidence-based decision framework** for SMEs selecting cloud data warehouses

---

## ğŸ” Research Gap Addressed

* Conflicting benchmark results in existing literature
* Lack of neutral, tri-platform comparisons under controlled configurations
* Limited practical guidance for SMEs balancing cost, performance, and operational complexity

---

## ğŸ§ª Methodology

A **seven-phase experimental pipeline** was adopted, featuring:

* **Benchmarks:**

  * TPC-DS (99 queries, 24 tables)
  * TPC-H (22 queries, 8 tables)
* **Scale Factor:** 10GB (SME-representative)
* **Execution Model:** Cold and warm run separation
* **Iterations:** 20 executions per query

### ğŸ“Š Statistical Analysis

* **Kruskalâ€“Wallis H-test** (Î± = 0.05)
* **Dunnâ€™s post-hoc pairwise comparisons**
* **Cliffâ€™s Delta effect sizes** for practical significance

---

## ğŸš€ Key Findings

* **Oracle ADW** delivered the lowest median latency and exceptional consistency, with **12.2Ã— concurrency scaling**.
* **Amazon Redshift** showed competitive performance, particularly on TPC-H workloads, but required manual DBA tuning.
* **Azure Synapse (DW200c)** proved inadequate for production workloads, with optimization often degrading performance due to resource constraints.

**Key Insight:** Optimization effectiveness is **highly platform- and resource-dependent**. Autonomous optimization consistently outperformed manual tuning under multi-user workloads.

---

## ğŸ§  Contributions

âœ” Neutral tri-platform benchmarking under controlled conditions
âœ” Explicit measurement of optimization marginal effects
âœ” Telemetry-driven performance root cause analysis
âœ” Practical, SME-focused decision framework

---

## âš ï¸ Limitations

* 10GB scale factor limits enterprise-scale generalization
* Single-region deployment (US-East)
* Temporal validity (October 2025 snapshot)

---

## ğŸ”® Future Work

* Larger scale factors (SF100â€“SF1000)
* Multi-region and cost-per-query analysis
* Real-world workload traces
* Evaluation of Azure Synapse Spark pools

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ datasets/          # TPC-DS and TPC-H data generation scripts
â”œâ”€â”€ sql/               # Benchmark and optimization SQL queries
â”œâ”€â”€ results/           # Raw and aggregated performance results
â”œâ”€â”€ analysis/          # Statistical analysis notebooks
â”œâ”€â”€ figures/           # Charts and plots used in dissertation
â””â”€â”€ README.md          # Project overview
```

---

## ğŸ“– Citation

If you use this work, please cite it appropriately as part of academic or research outputs.

---

â­ *This repository supports MSc-level research into cloud data warehousing performance and optimization.*
